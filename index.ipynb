{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![PyPI version](https://badge.fury.io/py/llamass.svg)](https://badge.fury.io/py/llamass)\n",
    "\n",
    "\n",
    "![example workflow](https://github.com/gngdb/llamass/workflows/CI/badge.svg)\n",
    "\n",
    "\n",
    "# llamass\n",
    "\n",
    "> A Light Loader for the [AMASS dataset][amass] to make downloading and training on it easier.\n",
    "\n",
    "I'm writing this to use in a project working with pose data. I wanted to be able to install it in colab notebooks and elsewhere easily. Hopefully it's also useful for other people but be aware this is research code so not necessarily reliable.\n",
    "\n",
    "[amass]: https://amass.is.tue.mpg.de/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "* ~~Note here about the dataset license~~\n",
    "* ~~Instructions on how to download the dataset~~\n",
    "* Instructions on how to install the requirements for visualization\n",
    "* Augmentations pulled from original AMASS repo\n",
    "* ~~Install nbqa and black, run on existing notebooks~~\n",
    "* Example train/test splits by unpacking different datasets to different locations\n",
    "* ~~Add CC licensed picture of llamas to github preview~~\n",
    "* ~~add to pypi~~\n",
    "* ~~add badges~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License Agreement\n",
    "\n",
    "Before using the AMASS dataset I'm expected to sign up to the license agreeement [here][amass]. This package doesn't require any other code from MPI but visualization of pose data does, see below.\n",
    "\n",
    "### Install with pip\n",
    "\n",
    "Requirements are handled by pip during the install but in a new environment I would install [pytorch][]\n",
    "first to install it with cuda.\n",
    "\n",
    "`pip install llamass`\n",
    "\n",
    "### For Visualization\n",
    "\n",
    "**To do**: provide script to install this and all its requirements (for curl into bash on colab would be nice)\n",
    "\n",
    "* [Human Body Prior][hbp], licensed under the [SMPL-X project][smplx]\n",
    "* [Body Visualizer][body], licensed under the [SMPL-X project][smplx]\n",
    "* MAYBE [mesh][], does not require a sign up page\n",
    "\n",
    "For [MPI's mesh library][mesh], `libboost-dev` is required:\n",
    "\n",
    "```\n",
    "sudo apt-get install libboost-dev\n",
    "```\n",
    "\n",
    "[hbp]: https://github.com/nghorbani/human_body_prior\n",
    "[pytorch]: https://pytorch.org/get-started/locally/\n",
    "[amassrepo]: https://github.com/nghorbani/amass/blob/master/notebooks/01-AMASS_Visualization.ipynb\n",
    "[body]: https://github.com/nghorbani/body_visualizer\n",
    "[smplx]: https://smpl-x.is.tue.mpg.de/\n",
    "[mesh]: https://github.com/MPI-IS/mesh\n",
    "[amass]: https://amass.is.tue.mpg.de/index.html\n",
    "[pytables]: https://www.pytables.org/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data\n",
    "\n",
    "The [AMASS website][amass] provides links to download the various parts of the AMASS dataset. Each is provided as a `.tar.bz2` and I had to download them from the website by hand. Save all of these in a folder somehwere.\n",
    "\n",
    "### Unpacking the data\n",
    "\n",
    "After installing `llamass` a console script is provided to unpack the `tar.bz2` files downloaded from the [AMASS][] website:\n",
    "\n",
    "```\n",
    "fast_amass_unpack -n 4 <.tar.bz2 directory> <directory to save unpacked data>\n",
    "```\n",
    "\n",
    "This will unpack the data in parallel in 4 jobs and provides a progress bar.\n",
    "\n",
    "Alternatively, this can be access in the library using the `llamass.core.unpack_body_models` function:\n",
    "\n",
    "[amass]: https://amass.is.tue.mpg.de/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "unpacked_directory = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp2mpzo7r2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2122be782ba841e0816ae5aa19e362c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import llamass.core\n",
    "\n",
    "llamass.core.unpack_body_models(\"sample_data/\", unpacked_directory, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the data\n",
    "\n",
    "Once the data is unpacked it can be loaded by a PyTorch DataLoader directly using the `llamass.core.AMASS` Dataset class.\n",
    "\n",
    "* `overlapping`: whether the clips of frames taken from each file should be allowed to overlap\n",
    "* `clip_length`: how long should clips from each file be?\n",
    "* `transform`: a transformation function apply to all fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "amass = llamass.core.AMASS(\n",
    "    unpacked_directory, overlapping=False, clip_length=1, transform=torch.tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poses torch.Size([4, 1, 156])\n",
      "dmpls torch.Size([4, 1, 8])\n",
      "trans torch.Size([4, 1, 3])\n",
      "betas torch.Size([4, 1, 16])\n",
      "gender torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "amassloader = DataLoader(amass, batch_size=4, shuffle=True)\n",
    "\n",
    "for data in amassloader:\n",
    "    for k in data:\n",
    "        print(k, data[k].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "shutil.rmtree(unpacked_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "Caching the dataset may be easy to implement with [joblib's Memory][memory] so I'm looking into this.\n",
    "\n",
    "[memory]: https://joblib.readthedocs.io/en/latest/generated/joblib.Memory.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
