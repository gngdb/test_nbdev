{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Unpack and load the [AMASS][] dataset for training with a PyTorch iterator.\n",
    "\n",
    "To do:\n",
    "\n",
    "1. Saw `AMASS` and `global_index_map` break with relative paths, add test and make fix for this case\n",
    "2. Add options to pass `keep` index to `AMASS`\n",
    "\n",
    "[amass]: https://amass.is.tue.mpg.de/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpack Tar Files\n",
    "\n",
    "> Console script to unpack all tar files found in a specified directory and put them in another directory, then create a symlink to be able to find the unpacked data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import argparse\n",
    "import os\n",
    "from shutil import unpack_archive\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class ProgressParallel(joblib.Parallel):\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with tqdm(total=kwargs[\"total\"]) as self._pbar:\n",
    "            del kwargs[\"total\"]\n",
    "            return joblib.Parallel.__call__(self, *args, **kwargs)\n",
    "\n",
    "    def print_progress(self):\n",
    "        self._pbar.total = self.n_dispatched_tasks\n",
    "        self._pbar.n = self.n_completed_tasks\n",
    "        self._pbar.refresh()\n",
    "\n",
    "\n",
    "def unpack_body_models(tardir, outdir, n_jobs=1):\n",
    "    tar_root, _, tarfiles = [x for x in os.walk(tardir)][0]\n",
    "    tarfiles = [x for x in tarfiles if \"tar\" in x.split(\".\")]\n",
    "    tarpaths = [os.path.join(tar_root, tar) for tar in tarfiles]\n",
    "    for tarpath in tarpaths:\n",
    "        print(f\"{tarpath} extracting to {outdir}\")\n",
    "    ProgressParallel(n_jobs=n_jobs)(\n",
    "        (joblib.delayed(unpack_archive)(tarpath, outdir) for tarpath in tarpaths),\n",
    "        total=len(tarpaths),\n",
    "    )\n",
    "\n",
    "\n",
    "def fast_amass_unpack():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Unpack all the body model tar files in a directory to a target directory\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"tardir\",\n",
    "        type=str,\n",
    "        help=\"Directory containing tar.bz2 body model files\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"outdir\",\n",
    "        type=str,\n",
    "        help=\"Output directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-n\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Number of jobs to run the tar unpacking with\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    unpack_body_models(args.tardir, args.outdir, n_jobs=args.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test unpacking the sample data always yields the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp7wtjmhdp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7a53acf5154ba4bdf5c9d1fc7cf7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tempfile\n",
    "import hashlib\n",
    "\n",
    "# https://stackoverflow.com/a/3431838/6937913\n",
    "def md5(fname):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "\n",
    "md5sums = {\n",
    "    \"amass_sample.npz\": \"d0b546b3619c8579ade39e3a8ccdc4e2\",\n",
    "    \"dmpl_sample.npz\": \"576bb76b2a6328dc5c276c4150c466f0\",\n",
    "}\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    for r, d, f in os.walk(tmpdirname):\n",
    "        npz_files = [x for x in f if \"npz\" in x.split(\".\")]\n",
    "        npz_paths = [os.path.join(tmpdirname, r, x) for x in npz_files]\n",
    "    _md5sums = {os.path.split(fpath)[-1]: md5(fpath) for fpath in npz_paths}\n",
    "\n",
    "for k in md5sums:\n",
    "    assert md5sums[k] == _md5sums[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Functions\n",
    "\n",
    "> Load the pose data directly from the `npz` files after unpacking.\n",
    "\n",
    "Based on the [AMASS tutorial notebooks][amass], I would like to iterate over the dataset using a PyTorch Dataloader.\n",
    "\n",
    "Steps to load:\n",
    "\n",
    "1. Enumerate the paths to all the `npz` files\n",
    "2. Inspect files for frame data\n",
    "3. Map from a global dataset index to indexes for each frame clip\n",
    "\n",
    "[amass]: https://github.com/nghorbani/amass/tree/master/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpsd5dr57v\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495314680957492eac7e213a1ad5fba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpsd5dr57v/sample/subdir/amass_sample.npz\n",
      "   ['poses', 'gender', 'mocap_framerate', 'betas', 'marker_data', 'dmpls', 'marker_labels', 'trans']\n",
      "   [('poses', (601, 156)), ('gender', ()), ('mocap_framerate', ()), ('betas', (16,)), ('marker_data', (601, 85, 3)), ('dmpls', (601, 8)), ('marker_labels', (85,)), ('trans', (601, 3))]\n",
      "/tmp/tmpsd5dr57v/sample/subdir/dmpl_sample.npz\n",
      "   ['poses', 'gender', 'mocap_framerate', 'betas', 'marker_data', 'dmpls', 'marker_labels', 'trans']\n",
      "   [('poses', (235, 156)), ('gender', ()), ('mocap_framerate', ()), ('betas', (16,)), ('marker_data', (235, 67, 3)), ('dmpls', (235, 8)), ('marker_labels', (67,)), ('trans', (235, 3))]\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    for r, d, f in os.walk(tmpdirname):\n",
    "        npz_files = [x for x in f if \"npz\" in x.split(\".\")]\n",
    "        npz_paths = [os.path.join(tmpdirname, r, x) for x in npz_files]\n",
    "    for npz_path in npz_paths:\n",
    "        cdata = np.load(npz_path)\n",
    "        print(npz_path)\n",
    "        print(\"  \", [k for k in cdata.keys()])\n",
    "        print(\"  \", [(k, cdata[k].shape) for k in cdata.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viable Indexes\n",
    "\n",
    "For every `npz` file I need to pull out the viable indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def viable_slice(cdata, keep):\n",
    "    \"\"\"\n",
    "    Inspects a dictionary loaded from `.npz` numpy dumps\n",
    "    and creates a slice of the viable indexes.\n",
    "    args:\n",
    "\n",
    "        - `cdata`: dictionary containing keys:\n",
    "            ['poses', 'gender', 'mocap_framerate', 'betas',\n",
    "             'marker_data', 'dmpls', 'marker_labels', 'trans']\n",
    "        - `keep`: ratio of the file to keep, between zero and 1.,\n",
    "            drops leading and trailing ends of the arrays\n",
    "\n",
    "    returns:\n",
    "\n",
    "        - viable: slice that can access frames in the arrays:\n",
    "            cdata['poses'], cdata['marker_data'], cdata['dmpls'], cdata['trans']\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        keep > 0.0 and keep <= 1.0\n",
    "    ), \"Proportion of array to keep must be between zero and one\"\n",
    "    n = cdata[\"poses\"].shape[0]\n",
    "    drop = (1.0 - keep) / 2.0\n",
    "    return slice(int(n * drop), int(n * keep + n * drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp17pnar6l\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d549fa5ca44f61a67999c643ceb84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp17pnar6l/sample/subdir/amass_sample.npz\n",
      "   slice(60, 540, None)\n",
      "/tmp/tmp17pnar6l/sample/subdir/dmpl_sample.npz\n",
      "   slice(23, 211, None)\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    for r, d, f in os.walk(tmpdirname):\n",
    "        npz_files = [x for x in f if \"npz\" in x.split(\".\")]\n",
    "        npz_paths = [os.path.join(tmpdirname, r, x) for x in npz_files]\n",
    "    for npz_path in npz_paths:\n",
    "        cdata = np.load(npz_path)\n",
    "        print(npz_path)\n",
    "        print(\"  \", viable_slice(cdata, 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Global Index to File Indexes\n",
    "\n",
    "I need to be able to map from a global dataset index to contiguous sets of frames in each file. Options:\n",
    "\n",
    "1. Whether the contiguous frames can overlap in different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def global_index_map(npz_directory, overlapping, clip_length, keep=0.8):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        - `npz_directory`: Directory containing `.npz` files\n",
    "        - `overlapping`: Whether clips can overlap\n",
    "        - `clip_length`:\n",
    "    returns:\n",
    "        - map from global index to corresponding file and array indexes\n",
    "    \"\"\"\n",
    "    for r, d, f in os.walk(npz_directory):\n",
    "        npz_files = [x for x in f if \"npz\" in x.split(\".\")]\n",
    "        npz_paths = [os.path.join(npz_directory, r, x) for x in npz_files]\n",
    "    # array slices for each file\n",
    "    viable_slices = {\n",
    "        npz_path: viable_slice(np.load(npz_path), keep=keep) for npz_path in npz_paths\n",
    "    }\n",
    "    # clip index -> array index\n",
    "    def clip_to_array_index(i, array_slice):\n",
    "        if not overlapping:\n",
    "            i = i * clip_length\n",
    "        return [i + array_slice.start + j for j in range(clip_length)]\n",
    "\n",
    "    # length of a slice\n",
    "    def lenslice(s):\n",
    "        if overlapping:\n",
    "            return (s.stop - s.start) - (clip_length - 1)\n",
    "        else:\n",
    "            return math.floor((s.stop - s.start) / clip_length)\n",
    "\n",
    "    # global index -> file, relative index\n",
    "    def find_array(i):\n",
    "        global_index, j = 0, 0\n",
    "        for npz_path in viable_slices:\n",
    "            global_index += lenslice(viable_slices[npz_path])\n",
    "            if i < global_index:\n",
    "                return npz_path, i - j\n",
    "            j = global_index\n",
    "\n",
    "    # how many examples are there in this dataset, total\n",
    "    n_examples = sum(lenslice(viable_slices[npz_path]) for npz_path in viable_slices)\n",
    "    # create map function\n",
    "    def global_to_array(i):\n",
    "        npz_path, j = find_array(i)\n",
    "        return npz_path, clip_to_array_index(j, viable_slices[npz_path])\n",
    "\n",
    "    return global_to_array, n_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for clip length 1 and non-overlapping clips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp1lm2xiom\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa81e1b1ed74b84832e44816844a19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 668\n"
     ]
    }
   ],
   "source": [
    "def test_global_index_map(clip_length, overlapping):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "        global_to_array, n_examples = global_index_map(\n",
    "            tmpdirname, overlapping=overlapping, clip_length=clip_length\n",
    "        )\n",
    "        print(f\"Number of examples in dataset: {n_examples}\")\n",
    "        count = 0\n",
    "        prev_j = [-1] * clip_length\n",
    "        for i in range(n_examples):\n",
    "            npz_path, j = global_to_array(i)\n",
    "            cdata = np.load(npz_path)\n",
    "            assert len(j) == clip_length\n",
    "            assert cdata[\"poses\"][j] is not None\n",
    "            assert all(k > 0 for k in j)\n",
    "            if not overlapping:\n",
    "                assert all(k not in prev_j for k in j)\n",
    "                prev_j = j\n",
    "            count += 1\n",
    "        assert count == n_examples\n",
    "\n",
    "\n",
    "test_global_index_map(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for clip length greater than 1 and non-overlapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp0tf97i2_\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ac3572136640949751933ff083e5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 222\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpbahxxekr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11480cdb2c6459395a8211e88abef3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 167\n"
     ]
    }
   ],
   "source": [
    "test_global_index_map(3, False)\n",
    "test_global_index_map(4, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for length of clip less than 1 and overlapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpdnfak_ly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b99cb0ca16b4dcf9e70e55529682d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 664\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpty_xfg6j\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe10eac4b0945ba887a2cf0d7aa3cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 662\n"
     ]
    }
   ],
   "source": [
    "test_global_index_map(3, True)\n",
    "test_global_index_map(4, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from `.npz` Files\n",
    "\n",
    "I want to load the data from the `.npz` files in a standard way, so I'm going to load each entry into its own array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_npz(npz_path, indexes):\n",
    "    cdata = np.load(npz_path)\n",
    "\n",
    "    # unpack and enforce data type\n",
    "    poses = cdata[\"poses\"][indexes].astype(np.float32)\n",
    "    dmpls = cdata[\"dmpls\"][indexes].astype(np.float32)\n",
    "    trans = cdata[\"trans\"][indexes].astype(np.float32)\n",
    "    betas = np.repeat(\n",
    "        cdata[\"betas\"][np.newaxis].astype(np.float32), repeats=len(indexes), axis=0\n",
    "    )\n",
    "\n",
    "    def gender_to_int(g):\n",
    "        # casting gender to integer will raise a warning in future\n",
    "        g = str(g.astype(str))\n",
    "        return {\"male\": -1, \"neutral\": 0, \"female\": 1}[g]\n",
    "\n",
    "    gender = np.array([gender_to_int(cdata[\"gender\"]) for _ in indexes])\n",
    "\n",
    "    return dict(poses=poses, dmpls=dmpls, trans=trans, betas=betas, gender=gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this works with different clip lengths and overlapping clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp522nzaaa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defdbeb9934a47dc8c8261a020343139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 156), (1, 8), (1, 3), (1, 16), (1,)]\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpd9q5dp3o\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73dcd74a54fd4d148ba3c6f8e3945546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 156), (3, 8), (3, 3), (3, 16), (3,)]\n",
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpbg16wz5r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d3fb727bc74cd8b6aba9f3345ea02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 156), (3, 8), (3, 3), (3, 16), (3,)]\n"
     ]
    }
   ],
   "source": [
    "def test_load_npz(clip_length, overlapping):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "        global_to_array, n_examples = global_index_map(\n",
    "            tmpdirname, overlapping=overlapping, clip_length=clip_length\n",
    "        )\n",
    "        npz_path, indexes = global_to_array(0)\n",
    "        data = load_npz(npz_path, indexes)\n",
    "        for k in data:\n",
    "            assert data[k].shape[0] == clip_length\n",
    "        print([data[k].shape for k in data])\n",
    "\n",
    "\n",
    "test_load_npz(1, False)\n",
    "test_load_npz(3, False)\n",
    "test_load_npz(3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset Class\n",
    "\n",
    "Creating a map-style PyTorch Dataset Class that uses these functions to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AMASS(Dataset):\n",
    "    def __init__(self, unpacked_directory, clip_length, overlapping, transform=None):\n",
    "        self.global_to_array, self.n_examples = global_index_map(\n",
    "            unpacked_directory, overlapping=overlapping, clip_length=clip_length\n",
    "        )\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = load_npz(*self.global_to_array(i))\n",
    "        return {k: self.transform(data[k]) for k in data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test I can load some data with this Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmp_4vxaxa5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6ba7b799cd492b84409e1f0ab3a656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poses torch.Size([1, 156])\n",
      "dmpls torch.Size([1, 8])\n",
      "trans torch.Size([1, 3])\n",
      "betas torch.Size([1, 16])\n",
      "gender torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    amass = AMASS(tmpdirname, overlapping=False, clip_length=1, transform=torch.tensor)\n",
    "    data = amass[0]\n",
    "    for k in data:\n",
    "        print(k, data[k].shape)\n",
    "        assert type(data[k]) is torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it works in a DataLoader to make batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data/sample.tar.bz2 extracting to /tmp/tmpq_71o724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d739e650c76447ebd18741891ebcc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poses torch.Size([4, 1, 156])\n",
      "dmpls torch.Size([4, 1, 8])\n",
      "trans torch.Size([4, 1, 3])\n",
      "betas torch.Size([4, 1, 16])\n",
      "gender torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 8)\n",
    "    amass = AMASS(tmpdirname, overlapping=False, clip_length=1, transform=torch.tensor)\n",
    "    amasstrain = DataLoader(amass, batch_size=4, shuffle=True)\n",
    "    for i, data in enumerate(amasstrain):\n",
    "        if i == 0:\n",
    "            for k in data:\n",
    "                print(k, data[k].shape)\n",
    "        assert data[\"poses\"].size(0) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
